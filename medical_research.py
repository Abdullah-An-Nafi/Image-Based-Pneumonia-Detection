# -*- coding: utf-8 -*-
"""Medical_Research.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10HFjkAsxjsOBC28iq4RgejyAwLmrEjbw
"""

#Image Preprocessing in Neural Networks and Deep Learning with Keras and TensorFlow

"""**About dataset**

collecting dataset:https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/discussion
"""

#collcting data from google drive
from google.colab import drive
drive.mount ('/content/drive')
#unzip file
from zipfile import ZipFile
file_name= '/content/drive/MyDrive/Online Doc Kaggle.zip'
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('finish')

"""**Implementation kears & tensorflow **

"""

!pip install tensorflow
import tensorflow as tf
print(tf.__version__)

!pip install Keras

"""Import TensorFlow and other necessary libraries"""

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers, regularizers
from tensorflow.keras.models import Sequential

"""**Load data using a Keras utility**"""

#create dataset
batch_size = 32
img_height = 180
img_width = 180
WEIGHT_DECAY = 0.001
LEARNING_RATE = 0.001
data_dir='/content/chest_xray/chest_xray/train'
train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

"""**Visualize the data**"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("on")

"""**Data augmentation**"""

data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
  ]
)

#Visualize the augmented data
plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
  for i in range(9):
    augmented_images = data_augmentation(images)
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_images[0].numpy().astype("uint8"))
    plt.axis("off")

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

inputs = keras.Input(shape=(180,180, 3))
x = layers.Conv2D(
    filters=32,
    kernel_size=3,
    padding="same",
    kernel_regularizer=regularizers.l2(WEIGHT_DECAY),
)(inputs)
x = layers.BatchNormalization()(x)
x = keras.activations.selu(x)
x = layers.Conv2D(64, 3, kernel_regularizer=regularizers.l2(WEIGHT_DECAY),)(x)
x = layers.BatchNormalization()(x)
x = keras.activations.selu(x)
x = layers.MaxPooling2D()(x)
x = layers.Conv2D(
    64, 3, activation="selu", kernel_regularizer=regularizers.l2(WEIGHT_DECAY),
)(x)
x = layers.Conv2D(128, 3, activation="selu",kernel_initializer="lecun_normal")(x)
x = layers.MaxPooling2D()(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation="selu",kernel_initializer="lecun_normal")(x)
x = layers.Dropout(0.5)(x)
x = layers.Dense(64, activation="selu",kernel_initializer="lecun_normal")(x)
output1 = layers.Dense(10, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=[output1])

"""**Compile the model**"""

model.compile(optimizer='Nadam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

epochs=500
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()